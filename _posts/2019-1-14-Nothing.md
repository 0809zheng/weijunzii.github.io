---
layout: post
title: '无题'
subtitle: '真的无题'
date: 2019-1-14
author: 伪君子
categories:
cover: ''
tags: Python

---

* content
{:toc}
# 0 前言
1 月 9 号张小龙在微信之夜上演讲了近 4 个小时，第二天就一堆文字稿出现了
![](https://upload-images.jianshu.io/upload_images/2989110-90cd0a9afef43c64.jpg)

图一，前面说的我没去看，但是应该都是对的；后面说张小龙说得最多的是哪些，我觉得不可能是这样的分布，于是去找了资料确认。

[采铜的原文中](https://mp.weixin.qq.com/s/hT45I5TsNv18AAfNsx81PA) 明确说到是『并不完全的统计』，为了做出和采铜不一样的并不完全的统计，我和采铜一样去腾讯公众号拿了[官方完整版文字稿](https://mp.weixin.qq.com/s/pLmuGoc4bZrMNl7MSoWgiA) 进行分词并且统计词频，。

请注意，我的分析是并不完全的、按照我的心思去掉数据的！！ 下图是出现频率较高的图。
![](https://upload-images.jianshu.io/upload_images/2989110-8bd3d5f42e478de2.png)
下图是部分和『底线』出现频率是一样的，没错，还有少，小，智能等，虽然我也没分析出什么来。
![](https://upload-images.jianshu.io/upload_images/2989110-560758e230fa803c.png)
为了显示我的不一样，我还做了几个词云图来配合我的演出。
![](https://upload-images.jianshu.io/upload_images/2989110-6ad43844d5857b88.png)
![](https://upload-images.jianshu.io/upload_images/2989110-b93ac26b1b1baec7.png)


想一想这些数据能不能自己统计出来，能不能以自己的方式表达数据背后的东西。为什么要这样？因为别人统计的数据不一定精准、清洗数据的方式和你的不一样、用的语言或者工具和你的也不一样。

也不是说别人的分析不能看，我只是建议自己分析，然后结合别人的分析一起看。想想数据清洗的时候别人是为什么去掉一些数据，为什么只把部分数据拿出来分析。

采铜的文章，从立意和分析都是值得去学习的，至于最开始发的那张图，当个笑话看就好。

#  1 代码
还是 Python 代码，还是先导入库
```Python
import matplotlib.pyplot as plt
import jieba.analyse
from wordcloud import WordCloud, ImageColorGenerator
import numpy as np
import PIL.Image as Image
```
C:/Users\ASUS\Desktop\歌词/l.txt 里面的内容是要进行统计词频的文字,下面代码的功能是读取出要统计的文字
```Python
fp = open('C:/Users\ASUS\Desktop\歌词/l.txt')
try:
    file_context = fp.read()
    #  file_context = open(file).read().splitlines()
finally:
    fp.close()
```
这部分代码的功能是分词，然后统计词频并且打印出来。可以对代码进行一下修改，把统计好的结果写入 txt 文件或者 excel 文件中。
```Python
words = jieba.cut(file_context, cut_all=True)
word_freq = {}
for word in words:
    if word in word_freq:
        word_freq[word] += 1
    else:
        word_freq[word] = 1
freq_word = []
for word, freq in word_freq.items():
    freq_word.append((word, freq))
freq_word.sort(key=lambda x: x[1], reverse = True)
max_number = int(100)  # 100可以修改成别的数，意思是只显示前 100 个词的词频
for word, freq in freq_word[: max_number]:
    print(word, freq)
```
下面这部分的代码是分词，然后把分词后的结果以图片的形式显示出来。
```Python
word_list = jieba.cut(file_context, cut_all=True)
word_space_split = ' '.join(word_list)

coloring = np.array(Image.open("E:/Python/wechat/toux.jpg"))  # 设置图片作为词云图的背景
my_wordcloud = WordCloud(background_color="white", max_words=2000,
                         mask=coloring, max_font_size=100, random_state=42, scale=2,
                         font_path="C:/Windows/Fonts/simkai.ttf").generate(word_space_split)
#  max_words 可以修改
image_colors = ImageColorGenerator(coloring)
plt.imshow(my_wordcloud.recolor(color_func=image_colors))
plt.imshow(my_wordcloud)
plt.axis("off")
plt.show()
```
这里没有过多的解释，因为可以不用写代码。

# 2 不写代码

在线网站，每一个网站都不一样，我就不一一介绍了。

拿到要分析的文本后自行动手，分词和词云图都有。
>[http://corpus.zhonghuayuwen.org/CpsTongji.aspx](http://corpus.zhonghuayuwen.org/CpsTongji.aspx)

>[http://picdata.cn/index.php](http://picdata.cn/index.php)

>[http://cloud.niucodata.com/](http://cloud.niucodata.com/)

>[https://minitagcloud.com/](https://minitagcloud.com/)

>[http://ictclas.nlpir.org/nlpir/](http://ictclas.nlpir.org/nlpir/)

# 3 说明
哪怕错过了 2019 微信公开课的直播，你还可以去看看回放，[腾讯大学](http://tencent.hlpwsy.cn/tencentVideoMobile/index-pc.html)那里就有视频，建议用电脑看。

注：我也不知道我写了些什么狗屁，感觉语句就没有通顺的，废话一堆，该说的却没说透，但是，就这样吧以后再改。